{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Model & Strategy Comparison\n",
    "This phase compares different classifiers (Logistic Regression, SVM, Naive Bayes) and tests the hypothesis of grouped vs. all classes for both clarity_label and evasion_label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from src.preprocessing import get_preprocessed_data\n",
    "from src.evaluate import run_full_evaluation\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "train_df, val_df, test_df, id2label, label2id = get_preprocessed_data()\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "print(f\"Evasion labels: {id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Grouping Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_evasion_labels(series):\n",
    "    \"\"\"\n",
    "    Group evasion labels into 3 main classes:\n",
    "    - Clear: Explicit\n",
    "    - Ambivalent Reply: General, Implicit, Dodging, Deflection\n",
    "    - Clear non-reply: Claims ignorance, Clarification, Declining to answer\n",
    "    - Partial/half-answer: Partial/half-answer\n",
    "    \"\"\"\n",
    "    # First convert integers back to string labels\n",
    "    string_labels = series.map(id2label)\n",
    "    \n",
    "    # Then apply grouping mapping\n",
    "    mapping = {\n",
    "        'Explicit': 'Clear',\n",
    "        'General': 'Ambivalent Reply',\n",
    "        'Implicit': 'Ambivalent Reply',\n",
    "        'Dodging': 'Ambivalent Reply',\n",
    "        'Claims ignorance': 'Clear non-reply',\n",
    "        'Clarification': 'Clear non-reply',\n",
    "        'Declining to answer': 'Clear non-reply',\n",
    "        'Partial/half-answer': 'Ambivalent Reply',\n",
    "        'Deflection': 'Ambivalent Reply',\n",
    "    }\n",
    "    return string_labels.map(mapping)\n",
    "\n",
    "# Create grouped versions\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['evasion_label_grouped'] = group_evasion_labels(df['evasion_label'], id2label)\n",
    "\n",
    "print(\"Evasion label distribution (all classes):\")\n",
    "print(train_df['evasion_label'].value_counts())\n",
    "print(\"\\nEvasion label distribution (grouped):\")\n",
    "print(train_df['evasion_label_grouped'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Best Features from Phase 2\n",
    "Based on Phase 2 results, we use:\n",
    "- Sub-question + Answer context\n",
    "- Meta-features (multiple_questions, affirmative_questions, inaudible)\n",
    "- Trigrams (1, 3) which performed best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contextual features\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['sub_q_context'] = df['question'].fillna('') + \" [SEP] \" + df['interview_answer'].fillna('')\n",
    "    \n",
    "meta_cols = ['multiple_questions', 'affirmative_questions', 'inaudible']\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for col in meta_cols:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "print(\"Features prepared!\")\n",
    "print(f\"Text feature: sub_q_context\")\n",
    "print(f\"Meta features: {meta_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier_experiment(\n",
    "    clf_name,\n",
    "    vectorizer_name,\n",
    "    label_column,\n",
    "    train_df,\n",
    "    val_df,\n",
    "    use_meta_features=True,\n",
    "    ngram_range=(1, 2)\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single classifier experiment\n",
    "    \n",
    "    Parameters:\n",
    "    - clf_name: 'logistic', 'svm', or 'naive_bayes'\n",
    "    - vectorizer_name: 'tfidf' or 'count'\n",
    "    - label_column: 'clarity_label', 'evasion_label', or 'evasion_label_grouped'\n",
    "    - use_meta_features: whether to include meta-features\n",
    "    - ngram_range: tuple for n-gram range\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get labels\n",
    "    label_column = 'evasion_label_grouped' if use_grouped else 'evasion_label'\n",
    "    y_train = train_df[label_column]\n",
    "    y_val = val_df[label_column]\n",
    "    \n",
    "    # Build vectorizer\n",
    "    vec_params = {\n",
    "        'max_features': 2000,\n",
    "        'ngram_range': ngram_range,\n",
    "        'min_df': 2,\n",
    "        'max_df': 0.95\n",
    "    }\n",
    "    \n",
    "    if vectorizer_name == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(**vec_params)\n",
    "    else:  # count\n",
    "        vectorizer = CountVectorizer(**vec_params)\n",
    "    \n",
    "    # Build classifier\n",
    "    if clf_name == 'logistic':\n",
    "        classifier = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", random_state=SEED)\n",
    "    elif clf_name == 'svm':\n",
    "        classifier = LinearSVC(max_iter=2000, class_weight='balanced', random_state=SEED, dual='auto')\n",
    "    else:  # naive_bayes\n",
    "        classifier = MultinomialNB()\n",
    "    \n",
    "    # Build pipeline\n",
    "    if use_meta_features:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', vectorizer, 'sub_q_context'),\n",
    "                ('meta', 'passthrough', meta_cols)\n",
    "            ]\n",
    "        )\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        X_train = train_df\n",
    "        X_val = val_df\n",
    "    else:\n",
    "        X_train_text = train_df['sub_q_context']\n",
    "        X_val_text = val_df['sub_q_context']\n",
    "        X_train = vectorizer.fit_transform(X_train_text)\n",
    "        X_val = vectorizer.transform(X_val_text)\n",
    "        pipeline = classifier\n",
    "    \n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'classifier': clf_name,\n",
    "        'vectorizer': vectorizer_name,\n",
    "        'label_column': label_column,\n",
    "        'use_meta': use_meta_features,\n",
    "        'f1_macro': f1_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_val, y_pred, average='weighted', zero_division=0),\n",
    "        'precision': precision_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "        'recall': recall_score(y_val, y_pred, average='macro', zero_division=0),\n",
    "        'accuracy': accuracy_score(y_val, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics, pipeline, y_pred\n",
    "\n",
    "print(\"Experiment runner ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Classifier Comparison on Clarity Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clarity = []\n",
    "\n",
    "classifiers = ['logistic', 'svm', 'naive_bayes']\n",
    "vectorizers = ['tfidf', 'count']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 1: Classifier Comparison on Clarity Label\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for clf_name in classifiers:\n",
    "    for vec_name in vectorizers:\n",
    "        print(f\"\\nTesting: {clf_name} + {vec_name}...\")\n",
    "        \n",
    "        metrics, _, _ = run_classifier_experiment(\n",
    "            clf_name=clf_name,\n",
    "            vectorizer_name=vec_name,\n",
    "            label_column='evasion_label',\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            use_meta_features=True\n",
    "        )\n",
    "        \n",
    "        results_clarity.append(metrics)\n",
    "        print(f\"  F1 Macro: {metrics['f1_macro']:.4f}\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "df_clarity = pd.DataFrame(results_clarity)\n",
    "df_clarity_sorted = df_clarity.sort_values('f1_macro', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY - Clarity Label\")\n",
    "print(\"=\"*80)\n",
    "print(df_clarity_sorted[['classifier', 'vectorizer', 'f1_macro', 'accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Grouped vs. All Classes for Evasion Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_evasion_comparison = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 2: Grouped vs. All Classes for Evasion Label\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test each classifier with both grouped and all classes\n",
    "for clf_name in classifiers:\n",
    "    for vec_name in vectorizers:\n",
    "        print(f\"\\n{clf_name} + {vec_name}:\")\n",
    "        \n",
    "        # All classes\n",
    "        metrics_all, _, _ = run_classifier_experiment(\n",
    "            clf_name=clf_name,\n",
    "            vectorizer_name=vec_name,\n",
    "            label_column='evasion_label',\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            use_meta_features=True\n",
    "        )\n",
    "        metrics_all['class_strategy'] = 'all_classes'\n",
    "        \n",
    "        # Grouped classes\n",
    "        metrics_grouped, _, _ = run_classifier_experiment(\n",
    "            clf_name=clf_name,\n",
    "            vectorizer_name=vec_name,\n",
    "            label_column='evasion_label_grouped',\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            use_meta_features=True\n",
    "        )\n",
    "        metrics_grouped['class_strategy'] = 'grouped'\n",
    "        \n",
    "        improvement = metrics_grouped['f1_macro'] - metrics_all['f1_macro']\n",
    "        \n",
    "        print(f\"  All classes:     F1 = {metrics_all['f1_macro']:.4f}\")\n",
    "        print(f\"  Grouped classes: F1 = {metrics_grouped['f1_macro']:.4f}\")\n",
    "        print(f\"  Improvement:     {improvement:+.4f}\")\n",
    "        \n",
    "        results_evasion_comparison.append(metrics_all)\n",
    "        results_evasion_comparison.append(metrics_grouped)\n",
    "\n",
    "df_evasion = pd.DataFrame(results_evasion_comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY - Evasion Label Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(df_evasion[['classifier', 'vectorizer', 'class_strategy', 'f1_macro', 'accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Direct Comparison - Grouped vs. All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 3: Grouped (3) vs. All (7) Classes - Direct Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for clf_name in classifiers:\n",
    "    for vec_name in vectorizers:\n",
    "        model_name = f\"{clf_name} + {vec_name}\"\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # Get F1 scores from previous experiments\n",
    "        f1_all = df_all_classes[\n",
    "            (df_all_classes['classifier'] == clf_name) & \n",
    "            (df_all_classes['vectorizer'] == vec_name)\n",
    "        ]['f1_macro'].values[0]\n",
    "        \n",
    "        f1_grouped = df_grouped[\n",
    "            (df_grouped['classifier'] == clf_name) & \n",
    "            (df_grouped['vectorizer'] == vec_name)\n",
    "        ]['f1_macro'].values[0]\n",
    "        \n",
    "        improvement = f1_grouped - f1_all\n",
    "        improvement_pct = (improvement / f1_all) * 100 if f1_all > 0 else 0\n",
    "        \n",
    "        print(f\"  All 7 classes:  F1 = {f1_all:.4f}\")\n",
    "        print(f\"  Grouped 3:      F1 = {f1_grouped:.4f}\")\n",
    "        print(f\"  Improvement:    {improvement:+.4f} ({improvement_pct:+.1f}%)\")\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'model': model_name,\n",
    "            'classifier': clf_name,\n",
    "            'vectorizer': vec_name,\n",
    "            'f1_all_7': f1_all,\n",
    "            'f1_grouped_3': f1_grouped,\n",
    "            'improvement': improvement,\n",
    "            'improvement_pct': improvement_pct\n",
    "        })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_comparison[['model', 'f1_all_7', 'f1_grouped_3', 'improvement', 'improvement_pct']].to_string(index=False))\n",
    "\n",
    "# Calculate average improvement\n",
    "avg_improvement = df_comparison['improvement'].mean()\n",
    "print(f\"\\nAverage improvement from grouping: {avg_improvement:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Grouped vs. All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for comparison plot\n",
    "comparison_data = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    for vec in vectorizers:\n",
    "        model_name = f\"{clf}\\n{vec}\"\n",
    "        \n",
    "        f1_all = df_evasion[\n",
    "            (df_evasion['classifier'] == clf) & \n",
    "            (df_evasion['vectorizer'] == vec) & \n",
    "            (df_evasion['class_strategy'] == 'all_classes')\n",
    "        ]['f1_macro'].values[0]\n",
    "        \n",
    "        f1_grouped = df_evasion[\n",
    "            (df_evasion['classifier'] == clf) & \n",
    "            (df_evasion['vectorizer'] == vec) & \n",
    "            (df_evasion['class_strategy'] == 'grouped')\n",
    "        ]['f1_macro'].values[0]\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'model': model_name,\n",
    "            'all_classes': f1_all,\n",
    "            'grouped': f1_grouped,\n",
    "            'improvement': f1_grouped - f1_all\n",
    "        })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot comparing F1 scores\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, df_comparison['all_classes'], width, label='All Classes', alpha=0.8, color='steelblue')\n",
    "ax1.bar(x + width/2, df_comparison['grouped'], width, label='Grouped (3 classes)', alpha=0.8, color='coral')\n",
    "\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('F1 Macro Score')\n",
    "ax1.set_title('Grouped vs. All Classes - Evasion Label')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df_comparison['model'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim(0, 1.0)\n",
    "\n",
    "# Improvement plot\n",
    "colors = ['green' if x > 0 else 'red' for x in df_comparison['improvement']]\n",
    "ax2.barh(df_comparison['model'], df_comparison['improvement'], color=colors, alpha=0.7)\n",
    "ax2.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('F1 Improvement (Grouped - All)')\n",
    "ax2.set_title('Grouping Impact per Model')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/phase3_grouped_vs_all_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "if df_comparison['improvement'].mean() > 0:\n",
    "    print(f\"✓ Grouping classes improves performance on average by {df_comparison['improvement'].mean():.4f}\")\n",
    "else:\n",
    "    print(f\"✗ Grouping classes decreases performance on average by {abs(df_comparison['improvement'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Best Model with Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = pd.concat([df_all_classes, df_grouped])\n",
    "all_results_sorted = all_results.sort_values('f1_macro', ascending=False)\n",
    "\n",
    "best_overall = all_results_sorted.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL BEST MODEL FOR EVASION DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Classifier:   {best_overall['classifier']}\")\n",
    "print(f\"Vectorizer:   {best_overall['vectorizer']}\")\n",
    "print(f\"Classes:      {best_overall['classes']}\")\n",
    "print(f\"Meta Features: {best_overall['use_meta']}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  F1 Macro:     {best_overall['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Weighted:  {best_overall['f1_weighted']:.4f}\")\n",
    "print(f\"  Precision:    {best_overall['precision']:.4f}\")\n",
    "print(f\"  Recall:       {best_overall['recall']:.4f}\")\n",
    "print(f\"  Accuracy:     {best_overall['accuracy']:.4f}\")\n",
    "\n",
    "# Train best model and show confusion matrix\n",
    "use_grouped = (best_overall['classes'] == 'grouped_3')\n",
    "_, best_pipeline, y_pred, y_true = run_classifier_experiment(\n",
    "    clf_name=best_overall['classifier'],\n",
    "    vectorizer_name=best_overall['vectorizer'],\n",
    "    use_grouped=use_grouped,\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    use_meta_features=True\n",
    ")\n",
    "\n",
    "# Plot confusion matrix\n",
    "labels = sorted(y_true.unique())\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels, normalize='true')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap='Blues', values_format='.2f')\n",
    "plt.title(f\"Best Model: {best_overall['classifier']} + {best_overall['vectorizer']}\\n\"\n",
    "          f\"{best_overall['classes']} | F1 Macro: {best_overall['f1_macro']:.4f}\",\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/phase3_best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# By classifier\n",
    "clf_comparison = all_results.groupby('classifier')['f1_macro'].mean().sort_values()\n",
    "clf_comparison.plot(kind='barh', ax=axes[0, 0], color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Average F1 by Classifier', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('F1 Macro')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# By vectorizer\n",
    "vec_comparison = all_results.groupby('vectorizer')['f1_macro'].mean().sort_values()\n",
    "vec_comparison.plot(kind='barh', ax=axes[0, 1], color='coral', alpha=0.7)\n",
    "axes[0, 1].set_title('Average F1 by Vectorizer', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('F1 Macro')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# By class strategy\n",
    "class_comparison = all_results.groupby('classes')['f1_macro'].mean()\n",
    "class_comparison.plot(kind='bar', ax=axes[1, 0], color=['skyblue', 'salmon'], alpha=0.7)\n",
    "axes[1, 0].set_title('Average F1 by Class Strategy', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('F1 Macro')\n",
    "axes[1, 0].set_xticklabels(['All 7 Classes', 'Grouped 3 Classes'], rotation=0)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Overall ranking\n",
    "top_5 = all_results_sorted.head(5)\n",
    "top_5_labels = [f\"{row['classifier']}\\n{row['vectorizer']}\\n{row['classes']}\" \n",
    "                for _, row in top_5.iterrows()]\n",
    "axes[1, 1].barh(range(len(top_5)), top_5['f1_macro'], color='lightgreen', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(top_5)))\n",
    "axes[1, 1].set_yticklabels(top_5_labels, fontsize=9)\n",
    "axes[1, 1].set_title('Top 5 Configurations', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('F1 Macro')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/phase3_overall_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to CSV\n",
    "df_all_classes.to_csv('results/phase3_all_7_classes.csv', index=False)\n",
    "df_grouped.to_csv('results/phase3_grouped_3_classes.csv', index=False)\n",
    "df_comparison.to_csv('results/phase3_comparison_grouped_vs_all.csv', index=False)\n",
    "all_results_sorted.to_csv('results/phase3_all_results.csv', index=False)\n",
    "\n",
    "# Save best model info\n",
    "with open('results/phase3_best_model.txt', 'w') as f:\n",
    "    f.write(\"BEST MODEL FOR EVASION DETECTION\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Classifier:    {best_overall['classifier']}\\n\")\n",
    "    f.write(f\"Vectorizer:    {best_overall['vectorizer']}\\n\")\n",
    "    f.write(f\"Classes:       {best_overall['classes']}\\n\")\n",
    "    f.write(f\"Meta Features: {best_overall['use_meta']}\\n\")\n",
    "    f.write(f\"\\nPerformance:\\n\")\n",
    "    f.write(f\"  F1 Macro:    {best_overall['f1_macro']:.4f}\\n\")\n",
    "    f.write(f\"  F1 Weighted: {best_overall['f1_weighted']:.4f}\\n\")\n",
    "    f.write(f\"  Precision:   {best_overall['precision']:.4f}\\n\")\n",
    "    f.write(f\"  Recall:      {best_overall['recall']:.4f}\\n\")\n",
    "    f.write(f\"  Accuracy:    {best_overall['accuracy']:.4f}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Results exported to results/ directory\")\n",
    "print(\"Figures saved to figures/ directory\")\n",
    "print(\"\\nPhase 3 complete! ✓\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
